{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tutorial is a hands-on introduction to the Rescale HTC framework. The framework is applied to solve a real-world use case, covers most of the Rescale HTC API, and discusses pros and cons of alternative solutions. Best practices and rationale behind them are highlighted. The reader is advised to go through the entire interactive tutorial step-by-step.\n",
    "\n",
    "The tutorial supplements the two other documents forming the Rescale HTC documentation: *The Rescale HTC Manual* and *The HTCCTL User Guide* (contact your Rescale representative to get access to these documents).\n",
    "\n",
    "# Problem setting\n",
    "\n",
    "We have a set of 5000 random images. The goal is to find an image that has the most faces in it. To accomplish this, we detect faces in each image, draw bounding boxes around them, and calculate the total number of faces. Then, we aggregate results and display the image with the most faces.\n",
    "\n",
    "The validation set of the [Common Objects in Context](https://cocodataset.org/#download) dataset (5000 images) is used as the input image set.\n",
    "\n",
    "## Face recognition algorithm\n",
    "\n",
    "For face recognition, we use the [Cascade Classification](https://docs.opencv.org/2.4/modules/objdetect/doc/cascade_classification.html) as implemented in [OpenCV](https://opencv.org/). Details of the face detection implementation are omitted. The implementation is encapsulated in the `face_detect` function of the [`face_tagger.py`](face_tagger/face_tagger.py). The function takes image path as input and, if faces are detected, writes two files: `{image}_tagged.dat` file containing the face count, and a `{image}_tagged.jpg` file with bounding boxes.\n",
    "\n",
    "# Solution\n",
    "\n",
    "A typical workflow for mapping a computational problem to Rescale HTC consists of the following steps:\n",
    "\n",
    "* Design work distribution approach\n",
    "* Implement and containerize application code\n",
    "* Create an HTC Project\n",
    "* Publish the container image\n",
    "* Create an HTC Task\n",
    "* Populate storage with inputs\n",
    "* Submit HTC Jobs\n",
    "* Aggregate results\n",
    "* Clean up resources\n",
    "\n",
    "The following sections implement each of these steps.\n",
    "\n",
    "## Distributing work\n",
    "\n",
    "Our approach to work distribution and storage interaction will affect our application code implementation, HTC project setup and submission. Therefore, we need to carefully design our work distribution approach.\n",
    "\n",
    "In our use case, we have 5000 images that need tagging. Our local run demonstrated that single detection, including input/output, takes about ~0.12 seconds. A single run for the entire image library would take about ~10 minutes. This may not be a good problem for HTC, but let's imagine that the full library has 5 million images. Here, we use a smaller input set to validate our design.\n",
    "\n",
    "It is a good practice to implement and test the entire workflow on a subset of inputs.\n",
    "\n",
    "The first thing we need to think about is where are we going to store inputs and outputs. Rescale HTC gives us an option to use project shared storage or task storage. Since our input image library is immutable, let's put it in project shared storage. We will use task storage to store results generated by jobs.\n",
    "\n",
    "The face detection algorithm used by our application can be parameterized by the scaling factor. We may at some point want to find the optimal value for this parameter. By using separate input and output storage, we can clearly differentiate between results obtained for different parameters. All experiments use the same input library (project shared storage), tasks encapsulate runs for different parameters. The task name can then carry contextual information, and task storage will keep results that are not mixed with other experiments.\n",
    "\n",
    "Next, we need to decide how are we going to split work between tasks. In an extreme case, we could specify image names as inputs to each job with `batchSize=1`. Submitting 5000 individual jobs prevents the Rescale HTC runtime from maximizing throughput. We should always think in terms of batches. Each job in a batch is aware of its `index`. We can use this `index` and let the job itself discover a chunk of work to process. We have several options here.\n",
    "\n",
    "Furthermore, we could split the list of image names into chunks, and store these for each job `index` before submission. Multiple `image_list.{index}` files could then be pushed to either project or task storage. A job instance would then fetch the image list using its `index`. Such files could have the following form:\n",
    "\n",
    "```\n",
    "$ cat image_list.1\n",
    "000000581357.jpg\n",
    "000000581482.jpg\n",
    "000000581615.jpg\n",
    "000000581781.jpg\n",
    "```\n",
    "\n",
    "The downside of this approach is that we need to regenerate the `image_list.{index}` files, every time we decide to change the batch size.\n",
    "\n",
    "An alternative solution would be to determine the chunk during runtime. Each job could list files in the project shared storage, sort them, and take a chunk of the resulting list based on its `index`. This is a valid approach, but it may go wrong if project storage becomes polluted.\n",
    "\n",
    "We will proceed with an approach where a file containing a list of all image names is uploaded to project shared storage. The name of this file is then passed to the job. A job fetches this file and determines its chunk based on `index` within a batch.\n",
    "\n",
    "## Containerization\n",
    "\n",
    "The Docker image contains executable code and its dependencies required to run the task.\n",
    "\n",
    "Our [Dockerfile](docker/Dockerfile) uses the official `python:3.10` container as a base, installs a few library dependencies and required Python modules, including the Rescale `htcctl` utility for interacting with object storage.\n",
    "\n",
    "The image also includes the [face_tagger.py](face_tagger/face_tagger.py) script, which implements the detection pipeline. We will discuss its structure later in this tutorial.\n",
    "\n",
    "Make sure your Docker Desktop is running. The following command should execute without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before publishing the container to our private HTC repository, it is always a good idea to build and test it locally. To build the image, execute the following (may take a couple of minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker build -f docker/Dockerfile -t rescale/opencv-htc-tutorial ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's detect some faces using our container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run --rm -v $(pwd)/face_tagger/:/data rescale/opencv-htc-tutorial \\\n",
    "    python /opt/face_tagger.py /data/test_images/2_faces.jpg\n",
    "! cat face_tagger/test_images/2_faces.jpg_tagged.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our script detected 2 faces and output this information both, in the console,\n",
    "and in the `2_faces.jpg_tagged.dat` text file. Detected face regions are\n",
    "viusalised in the `2_faces.jpg_detect.jpg` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "display.Image(\"face_tagger/test_images/2_faces.jpg_tagged.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Rescale HTC API\n",
    "\n",
    "The API resources we are going to interact with are documented with [OpenAPI specification](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/). Use this documentation as the primary source of truth. It is always up to date and provides in-depth descriptions of models and operations. Refer to *The Rescale HTC Manual* for higher level explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication and authorization\n",
    "\n",
    "The Rescale API authorizes requests using an *API Key*. This key is unique per User, per Workspace. A give user may have multiple *API Keys* active - one per Workspace.\n",
    "\n",
    "To generate an *API Key*, go to the API section of the User Profile.\n",
    "\n",
    "![](README.assets/user_profile_apikey.png)\n",
    "\n",
    "> NOTE: Remember that your *API Key* is a secret. Never store it in a code repository. Never share it with your colleagues. If you realize that your *API Key* was exposed - delete it in the user profile and generate a new key.\n",
    "\n",
    "Rescale tools like [Rescale CLI](https://rescale.com/documentation/main-2/rescale-advanced-features/rescale-cli-1-1-x/setting-up-rescale-cli-110/) look for the `apiconfig` authorization configuration file in users' home directory.\n",
    "\n",
    "```\n",
    "$HOME/.config/rescale/apiconfig             # Linux\n",
    "%USERPROFILE%\\config\\rescale\\apiconfig      # Windows\n",
    "```\n",
    "\n",
    "We will use this convention to store our credentials. Create the `apiconfig` text file in the aforementioned location and fill it with the following lines\n",
    "\n",
    "```\n",
    "[default]\n",
    "apibaseurl = https://platform.rescale.com\n",
    "apikey = 79aaabb6477ffdddee56587697777abbbc66c6c9\n",
    "\n",
    "```\n",
    "\n",
    "The `config` module extracts the Rescale API Key from the `apiconfig` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "\n",
    "_, api_key = config.get_profile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Rescale HTC Bearer token has an expiration time of 6h. Depending on use case, it may be sensible to get a new token before each call. This especially applies to polling for job status of long-running job batches.\n",
    "\n",
    "Before obtaining the token, let's import all modules that will be used in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import datetime\n",
    "import os\n",
    "import time\n",
    "from pprint import pprint\n",
    "\n",
    "import jwt\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following call to the [`/auth/token`](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Token%20Resource/get_auth_token) endpoint returns an HTC Bearer token. Decoded `jwt` token contents are displayed for reference. Note the `groups` section, which lists roles associated with the owner of an HTC Bearer token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_res = requests.get(\n",
    "    \"https://htc.rescale.com/auth/token\",\n",
    "    headers={\"Authorization\": f\"Token {api_key}\"},\n",
    ")\n",
    "token_res.raise_for_status()\n",
    "\n",
    "decoded_token = jwt.decode(\n",
    "    token_res.json()[\"tokenValue\"], options={\"verify_signature\": False}\n",
    ")\n",
    "pprint(decoded_token, indent=4)\n",
    "HTC_TOKEN = token_res.json()[\"tokenValue\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project\n",
    "\n",
    "A project can use multiple regions and cloud providers to get higher throughput. Calling the [`/regions`](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Provider%20Resource/get_htc_regions) endpoint gives us available regions are supported platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_res = requests.get(\n",
    "    \"https://htc.rescale.com/api/v1/htc/regions\",\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    ")\n",
    "regions_res.raise_for_status()\n",
    "\n",
    "print(\n",
    "    *[\n",
    "        f\"{r['region']}: {', '.join(r['supportedArchitectures'])}\"\n",
    "        for r in regions_res.json()\n",
    "    ],\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above regions are compute regions. We can also find out, for which regions, storage containers have been enabled by calling the [`/storage`](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Storage%20Resource/get_htc_storage) endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_res = requests.get(\n",
    "    \"https://htc.rescale.com/api/v1/htc/storage\",\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    ")\n",
    "storage_res.raise_for_status()\n",
    "\n",
    "print(\n",
    "    *[\n",
    "        f\"{r['region']}: {r['storageName']}\"\n",
    "        for r in storage_res.json()\n",
    "    ],\n",
    "    sep=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new project, we call the [`/projects`](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Project%20Resource/get_htc_projects) endpoint. For simplicity, we chose to create a project in a single region that supports `AARCH64` and `X86` architectures, and has storage defined.\n",
    "\n",
    "Note that the `projectName` and `projectDescription` are not required to be unique. Every call to this endpoint will create a new project with unique `projectId`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"AWS_EU_WEST_1\"\n",
    "\n",
    "payload = {\n",
    "    \"projectName\": \"htc-tutorial\",\n",
    "    \"projectDescription\": \"Tutorial project\",\n",
    "    \"regions\": [ REGION ]\n",
    "}\n",
    "\n",
    "project_res = requests.post(\n",
    "    \"https://htc.rescale.com/api/v1/htc/projects\",\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    "    json=payload,\n",
    ")\n",
    "project_res.raise_for_status()\n",
    "pprint(project_res.json(), indent=4)\n",
    "\n",
    "PROJECT_ID = project_res.json()[\"projectId\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish Docker image\n",
    "\n",
    "By creating a new project, we also created an associated container registry. Let's [create a container image repository](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Container%20Registry%20Resource/post_htc_projects__projectId__container_registry_repo__repoName_). Note that if a repository already exists, the response property `successfullyCreated` will be set to `false`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPO_NAME = \"opencv-htc-tutorial\"\n",
    "IMAGE_TAG = \"v1\"\n",
    "\n",
    "repo_res = requests.post(\n",
    "    f\"https://htc.rescale.com/api/v1/htc/projects/{PROJECT_ID}/container-registry/repo/{REPO_NAME}\",\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    ")\n",
    "repo_res.raise_for_status()\n",
    "pprint(repo_res.json(), indent=4)\n",
    "\n",
    "REGISTRY_URI = repo_res.json()[\"registryURI\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can push images to the project's container registry, we need to [get the container registry token](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Container%20Registry%20Resource/get_htc_projects__projectId__container_registry_token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_token_res = requests.get(\n",
    "    f\"https://htc.rescale.com/htc/projects/{PROJECT_ID}/container-registry/token\",\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    ")\n",
    "docker_token_res.raise_for_status()\n",
    "\n",
    "DOCKER_TOKEN = docker_token_res.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now login with the Docker registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_netloc = REGISTRY_URI.split(\"/\")[0]\n",
    "! echo $DOCKER_TOKEN | docker login --username AWS --password-stdin $repo_netloc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build our Docker image for both architectures supported in our project region (docker uses  architecture labels: `amd64` and `arm64`, that differ from region labels). Cross compilation may be very slow. In production environments, where support for both `amd64` and `arm64` is required, it may make sense to pre-build required images using internal CI/CD systems which execute builds on build agents that match target architectures. The [Docker Buildx](https://docs.docker.com/build/building/multi-platform/) plugin automates multi-platform image creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker buildx create --name htcbuilder --use --bootstrap\n",
    "! docker buildx build --platform linux/amd64,linux/arm64 \\\n",
    "    -f docker/Dockerfile -t $REGISTRY_URI$REPO_NAME:$IMAGE_TAG --push ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can query the repository to check whether the manifest list for our images shows support for both architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker manifest inspect $REGISTRY_URI$REPO_NAME:$IMAGE_TAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also [get all container images](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Container%20Registry%20Resource/get_htc_projects__projectId__container_registry_images) registered in the project's container registry. We should see a tagged image `opencv-htc-tutorial:v1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_res = requests.get(\n",
    "    f\"https://htc.rescale.com/api/v1/htc/projects/{PROJECT_ID}/container-registry/images\",\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    ")\n",
    "images_res.raise_for_status()\n",
    "pprint(images_res.json(), indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload input files\n",
    "\n",
    "The image library we will use as input is publicly available. Let's download and extract it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -O http://images.cocodataset.org/zips/val2017.zip\n",
    "! unzip -qo val2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways of interacting with the Rescale HTC cloud storage. The first one utilizes the `htcctl` command line tool, which abstracts away cloud provider-specific details and simplifies file uploads/downloads in a multi cloud setting. This should be the default choice for most cases.\n",
    "\n",
    "The second one uses cloud provider native tools that can be used with cloud provider-specific credentials and storage properties exposed by the HTC API. This should be used only in special cases.\n",
    "\n",
    "As discussed at the beginning of the tutorial, we want to upload the input dataset to the project shared storage. Note that `htcctl` keeps path to files in uploaded object key name. To strip the `val2017/` from object key name - then we need to invoke the command from within the `val2017/` directory. Refer to *The HTCCTL User Guide* for more information on `htcctl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pushd val2017\n",
    "! RESCALE_API_TOKEN=$api_key \\\n",
    "    htcctl --project-shared --project-id $PROJECT_ID \\\n",
    "        upload *\n",
    "%popd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following our approach to work distribution, let's create a file listing all image file names and upload it to the project shared storage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -1 val2017/ > image_list.dat\n",
    "! RESCALE_API_TOKEN=$api_key \\\n",
    "    htcctl --project-shared --project-id $PROJECT_ID \\\n",
    "        upload image_list.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Task\n",
    "\n",
    "Now that we have our input files in the project's object storage, we can proceed with [creating a task](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Task%20Resource/post_htc_projects__projectId__tasks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"taskName\": \"htc-task-from-htctutorial\",\n",
    "    \"taskDescription\": \"HTC Task from HTC Tutorial\",\n",
    "}\n",
    "\n",
    "task_res = requests.post(\n",
    "    f\"https://htc.rescale.com/api/v1/htc/projects/{PROJECT_ID}/tasks\",\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    "    json=payload,\n",
    ")\n",
    "task_res.raise_for_status()\n",
    "pprint(task_res.json(), indent=4)\n",
    "TASK_ID = task_res.json()[\"taskId\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating HTC runtime context locally \n",
    "\n",
    "Rescale HTC runtime injects environmental variables that establish a context for a job being executed. It is possible to set these locally, so we can troubleshoot and resolve issues without having to wait for a result of a job submission. We will use variables set by the previous operations and use their values to populate `RESCALE_HTC_` variables. The `HTC_INPUT_IMAGE` is a custom variable expected by our application. The `RESCALE_HTC_` variables are used by the `htcctl` tool. We are going to pull the docker image directly from our project's docker registry.\n",
    "\n",
    "> NOTE: The actual token injected into a container during runtime is a Job Bearer Token, which has a reduced set of privileges compared to the User Bearer Token. Consult the `Job Bearer Tokens` section of `The Rescale HTC Manual`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker pull $REGISTRY_URI$REPO_NAME:$IMAGE_TAG\n",
    "! docker run --rm \\\n",
    "    -e RESCALE_HTC_PROJECT_ID=$PROJECT_ID \\\n",
    "    -e RESCALE_HTC_TASK_ID=$TASK_ID \\\n",
    "    -e RESCALE_HTC_BEARER_TOKEN=$HTC_TOKEN \\\n",
    "    -e HTC_INPUT_IMAGE=true \\\n",
    "        $REGISTRY_URI$REPO_NAME:$IMAGE_TAG python /opt/face_tagger.py 000000367818.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing objects in the task storage should yield two result files (tagged image and a text file with face count)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! RESCALE_API_TOKEN=$api_key \\\n",
    "    htcctl --project-id $PROJECT_ID --task-id $TASK_ID \\\n",
    "        list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now download the tagged image and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_FILE = \"000000367818.jpg_tagged.jpg\"\n",
    "\n",
    "! RESCALE_API_TOKEN=$api_key \\\n",
    "    htcctl --project-id $PROJECT_ID --task-id $TASK_ID \\\n",
    "        download $RESULT_FILE\n",
    "\n",
    "display.Image(RESULT_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and run jobs\n",
    "\n",
    "Before submitting a batch of jobs, it is a good practice to [submit a single job](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Job%20Resource/post_htc_projects__projectId__tasks__taskId__jobs_batch) of `batchSize=1` just to confirm that everything works fine in the context of the Rescale HTC runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = [ {\n",
    "    \"jobName\": \"Test HTC tutorial job\",\n",
    "    \"batchSize\": 1,\n",
    "    \"htcJobDefinition\": {\n",
    "        \"imageName\": \"opencv-htc-tutorial:v1\",\n",
    "        \"maxVCpus\": 1,\n",
    "        \"maxMemory\": 128,\n",
    "        \"commands\": [\"python\", \"/opt/face_tagger.py\"],\n",
    "        \"envs\": [{\"name\": \"HTC_INPUT_IMAGE\", \"value\": \"true\"}],\n",
    "        \"execTimeoutSeconds\": 5000,\n",
    "    },\n",
    "} ]\n",
    "\n",
    "batch_res = requests.post(\n",
    "    f\"https://htc.rescale.com/api/v1/htc/projects/{PROJECT_ID}/tasks/{TASK_ID}/jobs/batch\",\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    "    json=payload,\n",
    ")\n",
    "batch_res.raise_for_status()\n",
    "pprint(batch_res.json(), indent=4)\n",
    "\n",
    "PARENT_JOB_ID = batch_res.json()[0][\"parentJobId\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of approaches to job status tracking. We will start from polling for [job instance events](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Job%20Resource/get_htc_projects__projectId__tasks__taskId__jobs__jobId__events). We want to poll for terminal states `FAILED` or `SUCCEEDED`. Job statuses are refreshed every 30 seconds, therefore, polling with higher frequency does not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poll_for_status_events(htc_token, project_id, task_id, parent_job_id, batch_index):\n",
    "    while True:\n",
    "        events_res = requests.get(\n",
    "            f\"https://htc.rescale.com/api/v1/htc/projects/{project_id}/tasks/{task_id}/jobs/{parent_job_id}:{batch_index}/events\",\n",
    "            headers={\"Authorization\": f\"Bearer {htc_token}\"},\n",
    "        )\n",
    "        events_res.raise_for_status()\n",
    "\n",
    "        statuses = [s[\"status\"] for s in events_res.json()[\"items\"]]\n",
    "        if \"FAILED\" in statuses or \"SUCCEEDED\" in statuses:\n",
    "            break\n",
    "\n",
    "        print(\"Waiting 30s for job to complete...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "    pprint(\n",
    "        [\n",
    "            s\n",
    "            for s in events_res.json()[\"items\"]\n",
    "            if s[\"status\"] in (\"FAILED\", \"SUCCEEDED\")\n",
    "        ][0],\n",
    "        indent=4,\n",
    "    )\n",
    "\n",
    "\n",
    "poll_for_status_events(HTC_TOKEN, PROJECT_ID, TASK_ID, PARENT_JOB_ID, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our job reached the `FAILED` terminal state and exited with non-zero `exitCode`. To aid troubleshooting, let's [fetch the job logs](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Job%20Resource/get_htc_projects__projectId__tasks__taskId__jobs__jobId__logs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logs(htc_token, project_id, task_id, parent_job_id, batch_index):\n",
    "    logs_res = requests.get(\n",
    "        f\"https://htc.rescale.com/api/v1/htc/projects/{project_id}/tasks/{task_id}/jobs/{parent_job_id}:{batch_index}/logs\",\n",
    "        headers={\"Authorization\": f\"Bearer {htc_token}\"},\n",
    "    )\n",
    "    logs_res.raise_for_status()\n",
    "\n",
    "    for i in logs_res.json()[\"items\"]:\n",
    "        print(f\"{i['timestamp']}\\t{i['message']}\")\n",
    "\n",
    "\n",
    "get_logs(HTC_TOKEN, PROJECT_ID, TASK_ID, PARENT_JOB_ID, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logs indicate that the application expected an additional command line argument. If the `HTC_INPUT_IMAGE` environmental variable is set, the application expects image name as an argument. Let's fix the `htcJobDefinition` and extend `commands` with image name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_payload = copy.deepcopy(payload)\n",
    "fixed_payload[0][\"htcJobDefinition\"][\"commands\"].append(\"000000367818.jpg\")\n",
    "\n",
    "fixed_batch_res = requests.post(\n",
    "    f\"https://htc.rescale.com/api/v1/htc/projects/{PROJECT_ID}/tasks/{TASK_ID}/jobs/batch\",\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    "    json=fixed_payload,\n",
    ")\n",
    "fixed_batch_res.raise_for_status()\n",
    "\n",
    "PARENT_JOB_ID = fixed_batch_res.json()[0][\"parentJobId\"]\n",
    "\n",
    "poll_for_status_events(HTC_TOKEN, PROJECT_ID, TASK_ID, PARENT_JOB_ID, 0)\n",
    "get_logs(HTC_TOKEN, PROJECT_ID, TASK_ID, PARENT_JOB_ID, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the real thing. We want to submit a batch of jobs that will process the full set of 5000 images. When splitting the workload into batches, we need to consider that a single batch is limited to 1000 job instances. This limitation allows the Rescale HTC runtime to maximize throughput by distributing jobs across cloud providers and regions. Our problem is small enough to fit into a single batch of jobs. The job submission endpoint accepts an array (of max size of 100) of batches allowing for creation of 100000 job instances with a single API call.\n",
    "\n",
    "As previously mentioned, we will utilize the `AWS_BATCH_JOB_ARRAY_INDEX` value injected by runtime to determine the chunk of images to be processed by a job instance (the `AWS_` prefixed envvar will also work for other CSPs, there is a limitation on the AWS side that forces us to use the `AWS_` prefixed name across all clouds). To allow jobs in a batch to determine the chunk of work to process at runtime, we will also inject our user-defined value `BATCH_JOB_ARRAY_SIZE` equal to the size of the batch. Note that environmental variables starting with `AWS_` are reserved and cannot be defined by users (`AWS_BATCH_JOB_ARRAY_SIZE` would not work).\n",
    "\n",
    "Before we launch the full job, it is a good practice to check if a run, simulating HTC environment will succeed. With batch size of 1000 and 5000 total images - we should have 5 images processed by a single run. Job output should show 5 images being downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run --rm \\\n",
    "    -e RESCALE_HTC_BEARER_TOKEN=$HTC_TOKEN \\\n",
    "    -e RESCALE_HTC_PROJECT_ID=$PROJECT_ID \\\n",
    "    -e RESCALE_HTC_TASK_ID=$TASK_ID \\\n",
    "    -e AWS_BATCH_JOB_ARRAY_INDEX=100 \\\n",
    "    -e IMG_OBJKEY_LIST=image_list.dat \\\n",
    "    -e BATCH_JOB_ARRAY_SIZE=1000 \\\n",
    "        $REGISTRY_URI$REPO_NAME:$IMAGE_TAG python /opt/face_tagger.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we launch the full batch, let's confirm that partial result files generated by our local run are present in the task storage. We are going to use AWS CLI to interact with S3 storage of our task. First, we need to [fetch credentials](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Task%20Resource/get_htc_projects__projectId__tasks__taskId__storage_token__region_) for the storage region we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_storage_token(htc_token, project_id, task_id, region):\n",
    "    regions_res = requests.get(\n",
    "        f\"https://htc.rescale.com/api/v1/htc/projects/{project_id}/tasks/{task_id}/storage/token/{region}\",\n",
    "        headers={\"Authorization\": f\"Bearer {htc_token}\"},\n",
    "    )\n",
    "    regions_res.raise_for_status()\n",
    "\n",
    "    return (\n",
    "        regions_res.json()[\"credentials\"][\"AWS_SESSION_TOKEN\"],\n",
    "        regions_res.json()[\"credentials\"][\"AWS_ACCESS_KEY_ID\"],\n",
    "        regions_res.json()[\"credentials\"][\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        regions_res.json()[\"storagePath\"],\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    AWS_SESSION_TOKEN,\n",
    "    AWS_ACCESS_KEY_ID,\n",
    "    AWS_SECRET_ACCESS_KEY,\n",
    "    STORAGE_S3_PATH,\n",
    ") = get_task_storage_token(HTC_TOKEN, PROJECT_ID, TASK_ID, REGION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AWS CLI expects credentials to be stored in environmental variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \\\n",
    "    AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\n",
    "        AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\n",
    "            aws s3 ls $STORAGE_S3_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to query [group summary statistics](https://htc.rescale.com/api-docs/ogrgmNBE+EiA4WoxiZRnHw==/#/Task%20Resource/get_htc_projects__projectId__tasks__taskId__group_summary_statistics) to track statuses of jobs within a group. The code polls for statuses until the sum of jobs in terminal states (`SUCCEEDED` or `FAILED`) is equal to the batch size. Status counts captured at polling intervals are then plotted to visualize sate transitions for the group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_group(project_id, task_id, group_name, group_size):\n",
    "    items = []\n",
    "    while True:\n",
    "        group_res = requests.get(\n",
    "            f\"https://htc.rescale.com/api/v1/htc/projects/{project_id}/tasks/{task_id}/group-summary-statistics\",\n",
    "            params={\"group\": group_name},\n",
    "            headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    "        )\n",
    "        group_res.raise_for_status()\n",
    "        job_statuses = group_res.json()[\"items\"][0][\"jobStatuses\"]\n",
    "\n",
    "        timestamp = datetime.datetime.now()\n",
    "        items.extend([[timestamp, s, job_statuses[s]] for s in job_statuses.keys()])\n",
    "\n",
    "        if job_statuses[\"SUCCEEDED\"] + job_statuses[\"FAILED\"] == group_size:\n",
    "            print(\"ALL DONE\")\n",
    "            pprint(job_statuses, indent=4)\n",
    "            break\n",
    "\n",
    "        print(\",\".join([f\"{s}: {job_statuses[s]}\" for s in job_statuses.keys()]))\n",
    "        print(\"Sleeping 30s...\")\n",
    "        time.sleep(30)\n",
    "\n",
    "    df = pd.DataFrame(items, columns=[\"ts\", \"status\", \"count\"])\n",
    "    fig = px.line(df, x=\"ts\", y=\"count\", color=\"status\", log_y=True, symbol=\"status\")\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's submit a batch of jobs. We will use the `group` query parameter to assign our batch to a group. This will allow us to track statuses for the entire group. Environmental variables `BATCH_JOB_ARRAY_SIZE` and `IMG_OBJKEY_LIST` are read by the application to determine the chunk of images to process by each job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1000\n",
    "GROUP_NAME = f\"htc-tutorial-jobgroup-{int(time.time())}\"\n",
    "\n",
    "batch_payload = [\n",
    "    {\n",
    "        \"jobName\": \"Test HTC tutorial job\",\n",
    "        \"batchSize\": BATCH_SIZE,\n",
    "        \"htcJobDefinition\": {\n",
    "            \"imageName\": \"opencv-htc-tutorial:v1\",\n",
    "            \"maxVCpus\": 1,\n",
    "            \"maxMemory\": 128,\n",
    "            \"commands\": [\"python\", \"/opt/face_tagger.py\"],\n",
    "            \"envs\": [\n",
    "                {\"name\": \"BATCH_JOB_ARRAY_SIZE\", \"value\": f\"{BATCH_SIZE}\"},\n",
    "                {\"name\": \"IMG_OBJKEY_LIST\", \"value\": \"image_list.dat\"},\n",
    "            ],\n",
    "            \"execTimeoutSeconds\": 5000,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "batch_res = requests.post(\n",
    "    f\"https://htc.rescale.com/api/v1/htc/projects/{PROJECT_ID}/tasks/{TASK_ID}/jobs/batch\",\n",
    "    params={\"group\": GROUP_NAME},\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    "    json=batch_payload,\n",
    ")\n",
    "batch_res.raise_for_status()\n",
    "pprint(batch_res.json(), indent=4)\n",
    "\n",
    "monitor_group(PROJECT_ID, TASK_ID, GROUP_NAME, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate results\n",
    "\n",
    "Our batch of jobs successfully completed. It is time to aggregate results. Let's fetch all the `.dat` files deposited in task storage. Note that storage tokens expire, we will fetch a new set of storage credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    AWS_SESSION_TOKEN,\n",
    "    AWS_ACCESS_KEY_ID,\n",
    "    AWS_SECRET_ACCESS_KEY,\n",
    "    STORAGE_S3_PATH,\n",
    ") = get_task_storage_token(HTC_TOKEN, PROJECT_ID, TASK_ID, REGION)\n",
    "\n",
    "! rm -fr val2017_output/*\n",
    "! AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN \\\n",
    "    AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID \\\n",
    "        AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY \\\n",
    "            aws s3 cp $STORAGE_S3_PATH val2017_output --recursive \\\n",
    "                --exclude \"*\" --include \"*.dat\" --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll process the `.dat` files to find the one with the highest count of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_faces(dir_path):\n",
    "    max_face_count = (None, 0)\n",
    "    for file in [f for f in os.listdir(dir_path) if f.endswith(\"dat\")]:\n",
    "        with open(f\"{dir_path}/{file}\", \"r\") as f:\n",
    "            face_cnt = int(f.readlines()[1])\n",
    "            if face_cnt > max_face_count[1]:\n",
    "                max_face_count = (file, face_cnt)\n",
    "    return max_face_count\n",
    "\n",
    "\n",
    "max_face_count = find_max_faces(\"val2017_output\")\n",
    "MAX_CNT_FILE = max_face_count[0].replace(\".dat\", \".jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fetch the image with the largest detected faces count and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! RESCALE_HTC_BEARER_TOKEN=$HTC_TOKEN RESCALE_HTC_REGION=$REGION \\\n",
    "    htcctl --project-id $PROJECT_ID --task-id $TASK_ID \\\n",
    "        download $MAX_CNT_FILE -d val2017_output/\n",
    "\n",
    "display.Image(f\"val2017_output/{MAX_CNT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "Now that we have successfully completed our task and found the image with the most faces in it, we can free up the storage resources by deleting the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_res = requests.delete(\n",
    "    f\"https://htc.rescale.com/api/v1/htc/projects/{PROJECT_ID}/tasks/{TASK_ID}\",\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"}\n",
    ")\n",
    "delete_res.raise_for_status()\n",
    "\n",
    "! rm -fr val2017*\n",
    "! find . -type f \\( -name \"*_tagged.*\" -o -name \"*.dat\" \\) -delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cancelling jobs\n",
    "\n",
    "We may encounter situations where something goes wrong with our jobs, our status monitoring shows unexpected number of failing jobs. Instead of waiting for all submitted jobs to complete, we can cancel the job group.\n",
    "\n",
    "Our application, when it detects that `SIMULATE_FAILURES` environmental variable, will randomly fail or timeout jobs. When we poll for status, at each interval, we calculate the percentage of failed jobs. If 15% of jobs are in a failed state - then job group is cancelled.\n",
    "\n",
    "The `/jobs` endpoint also allows for fetching jobs with specific statuses - we will use this later. Note that we can fetch either all statuses or filter for an individual status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_jobs(htc_token, project_id, task_id, group, status=None):\n",
    "    statuses = []\n",
    "\n",
    "    params = {\"group\": group, \"viewType\": \"SIMPLE\"}\n",
    "    if status:\n",
    "        params[\"status\"] = status\n",
    "\n",
    "    res = requests.get(\n",
    "        f\"https://htc.rescale.com/api/v1/htc/projects/{project_id}/tasks/{task_id}/jobs\",\n",
    "        params=params,\n",
    "        headers={\"Authorization\": f\"Bearer {htc_token}\"},\n",
    "    )\n",
    "    res.raise_for_status()\n",
    "    statuses.extend(res.json()[\"items\"])\n",
    "\n",
    "    while \"next\" in res.json():\n",
    "        res = requests.get(\n",
    "            res.json()[\"next\"], headers={\"Authorization\": f\"Bearer {htc_token}\"}\n",
    "        )\n",
    "        res.raise_for_status()\n",
    "        statuses.extend(res.json()[\"items\"])\n",
    "\n",
    "    return statuses\n",
    "\n",
    "\n",
    "def extract_status_counts(statuses):\n",
    "    status_counts = {\n",
    "        \"FAILED\": 0,\n",
    "        \"RUNNABLE\": 0,\n",
    "        \"RUNNING\": 0,\n",
    "        \"STARTING\": 0,\n",
    "        \"SUBMITTED_TO_PROVIDER\": 0,\n",
    "        \"SUBMITTED_TO_RESCALE\": 0,\n",
    "        \"SUCCEEDED\": 0,\n",
    "    }\n",
    "\n",
    "    for s in [i[\"status\"] for i in statuses]:\n",
    "        status_counts[s] += 1\n",
    "\n",
    "    return status_counts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function monitors job statuses using the above approach and decides whether a job group should be cancelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monitor_for_cancellation(htc_token, project_id, task_id, group, batch_size):\n",
    "    cancel = False\n",
    "    while True:\n",
    "        job_statuses = extract_status_counts(\n",
    "            get_all_jobs(htc_token, project_id, task_id, group)\n",
    "        )\n",
    "        timestamp = datetime.datetime.now()\n",
    "\n",
    "        succeeded = job_statuses[\"SUCCEEDED\"]\n",
    "        failed = job_statuses[\"FAILED\"]\n",
    "\n",
    "        print(f\"SUCCEEDED: {succeeded}, FAILED: {failed}. Waiting for 30s...\")\n",
    "\n",
    "        if succeeded + failed == batch_size:\n",
    "            print(\"ALL DONE\")\n",
    "            break\n",
    "        elif (succeeded + failed > 0) and (failed / (succeeded + failed) > 0.2):\n",
    "            print(\"TOO MANY FAILURES - CANCELLING...\")\n",
    "            cancel = True\n",
    "            break\n",
    "        time.sleep(30)\n",
    "\n",
    "    if cancel:\n",
    "        cancel_res = requests.post(\n",
    "            f\"https://htc.rescale.com/api/v1/htc/projects/{project_id}/tasks/{task_id}/jobs/cancel\",\n",
    "            params={\"group\": group},\n",
    "            headers={\"Authorization\": f\"Bearer {htc_token}\"},\n",
    "        )\n",
    "        cancel_res.raise_for_status()\n",
    "        monitor_group(project_id, task_id, group, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting a new batch, we need to create a new task, as the previous task was deleted. Our new task represents a separate experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_payload = {\n",
    "    \"taskName\": \"htc-task-from-htctutorial\",\n",
    "    \"taskDescription\": \"HTC Task from HTC Tutorial\",\n",
    "}\n",
    "\n",
    "task_res = requests.post(\n",
    "    f\"https://htc.rescale.com/api/v1/htc/projects/{PROJECT_ID}/tasks\",\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    "    json=task_payload,\n",
    ")\n",
    "task_res.raise_for_status()\n",
    "TASK_ID = task_res.json()[\"taskId\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's submit it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_payload = [\n",
    "    {\n",
    "        \"jobName\": \"Test HTC tutorial job\",\n",
    "        \"batchSize\": BATCH_SIZE,\n",
    "        \"htcJobDefinition\": {\n",
    "            \"imageName\": \"opencv-htc-tutorial:v1\",\n",
    "            \"maxVCpus\": 1,\n",
    "            \"maxMemory\": 128,\n",
    "            \"commands\": [\"python\", \"/opt/face_tagger.py\"],\n",
    "            \"envs\": [\n",
    "                {\"name\": \"SIMULATE_FAILURES\", \"value\": \"true\"},\n",
    "                {\"name\": \"BATCH_JOB_ARRAY_SIZE\", \"value\": f\"{BATCH_SIZE}\"},\n",
    "                {\"name\": \"IMG_OBJKEY_LIST\", \"value\": \"image_list.dat\"},\n",
    "            ],\n",
    "            \"execTimeoutSeconds\": 60,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "GROUP_NAME = f\"htc-tutorial-jobgroup-{int(time.time())}\"\n",
    "\n",
    "batch_res = requests.post(\n",
    "    f\"https://htc.rescale.com/api/v1/htc/projects/{PROJECT_ID}/tasks/{TASK_ID}/jobs/batch\",\n",
    "    params={\"group\": GROUP_NAME},\n",
    "    headers={\"Authorization\": f\"Bearer {HTC_TOKEN}\"},\n",
    "    json=fail_payload,\n",
    ")\n",
    "batch_res.raise_for_status()\n",
    "\n",
    "monitor_for_cancellation(HTC_TOKEN, PROJECT_ID, TASK_ID, GROUP_NAME, BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state transition plot shows that jobs are getting cancelled at different statuses. We expect some jobs to fail because of the artificial exception thrown by the application, some may have timed out, some will fail due to cancellation. Let's query for statuses of `FAILED` jobs and inspect failure codes and reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "failure_codes_reasons = {}\n",
    "for s in get_all_jobs(HTC_TOKEN, PROJECT_ID, TASK_ID, GROUP_NAME, \"FAILED\"):\n",
    "    code_reason = f\"{s['failureCode']}:{s['statusReason']}\"\n",
    "    if code_reason in failure_codes_reasons:\n",
    "        failure_codes_reasons[code_reason] += 1\n",
    "    else:\n",
    "        failure_codes_reasons[code_reason] = 0\n",
    "pprint(failure_codes_reasons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes the basic workflow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7379015c8f8310cee5ff1e59436d8f1ae8acbee80543bf6987cbe5c0d3aa2074"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
